{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "96fe3f21",
   "metadata": {},
   "source": [
    "# Learnable Alpha Pipeline Runner\n",
    "\n",
    "This notebook walks through executing the end-to-end learnable alpha workflow directly from Python, logging each shell command and summarising the generated artefacts."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f250104f",
   "metadata": {},
   "source": [
    "## 1. Validate Current Working Directory\n",
    "Confirm the notebook is pointed at the `MKA_improvement` folder before launching the pipeline commands."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ecf6d4d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Working directory: C:\\Users\\Parth\\OneDrive - iitgn.ac.in\\NLP\\Project\\MKA_improvement\n",
      "\n",
      "Key entries:\n",
      " - pipeline.py: exists\n",
      " - evaluate_methods.py: exists\n",
      " - plot_results.py: exists\n",
      " - data: exists\n",
      " - models: exists\n",
      " - experiments: exists\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "PROJECT_ROOT = Path.cwd().resolve()\n",
    "print(f\"Working directory: {PROJECT_ROOT}\")\n",
    "\n",
    "required_paths = [\n",
    "    \"pipeline.py\",\n",
    "    \"evaluate_methods.py\",\n",
    "    \"plot_results.py\",\n",
    "    \"data\",\n",
    "    \"models\",\n",
    "    \"experiments\",\n",
    "]\n",
    "print(\"\\nKey entries:\")\n",
    "for entry in required_paths:\n",
    "    status = \"exists\" if (PROJECT_ROOT / entry).exists() else \"missing\"\n",
    "    print(f\" - {entry}: {status}\")\n",
    "\n",
    "if PROJECT_ROOT.name != \"MKA_improvement\":\n",
    "    print(\"\\n[!] Consider changing directories so this notebook runs inside MKA_improvement/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e40dc3c3",
   "metadata": {},
   "source": [
    "## 2. Helper to Run and Log Shell Commands\n",
    "The helper below streams stdout/stderr so long-running jobs (e.g., LLM training) remain observable inside the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "764c3365",
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "import sys\n",
    "from datetime import datetime\n",
    "\n",
    "EXECUTE_COMMANDS = False  # Flip to True to actually run the heavy CLI commands.\n",
    "\n",
    "\n",
    "def run_command(command: str, *, cwd: Path | None = None) -> int:\n",
    "    \"\"\"Execute a shell command, streaming output line by line.\"\"\"\n",
    "    print(f\"\\n$ {command}\")\n",
    "    if not EXECUTE_COMMANDS:\n",
    "        print(\"[dry-run] EXECUTE_COMMANDS is False; command not executed.\")\n",
    "        return 0\n",
    "\n",
    "    process = subprocess.Popen(\n",
    "        command,\n",
    "        shell=True,\n",
    "        cwd=str(cwd) if cwd else None,\n",
    "        stdout=subprocess.PIPE,\n",
    "        stderr=subprocess.STDOUT,\n",
    "        text=True,\n",
    "        bufsize=1,\n",
    "        universal_newlines=True,\n",
    "    )\n",
    "\n",
    "    for line in process.stdout:\n",
    "        timestamp = datetime.now().strftime(\"%H:%M:%S\")\n",
    "        sys.stdout.write(f\"[{timestamp}] {line}\")\n",
    "\n",
    "    return_code = process.wait()\n",
    "    if return_code != 0:\n",
    "        raise RuntimeError(f\"Command failed with exit code {return_code}:\")\n",
    "    print(f\"\\nCommand completed successfully (exit code {return_code}).\")\n",
    "    return return_code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "956135cf",
   "metadata": {},
   "source": [
    "## 3. Launch Training Pipeline Command\n",
    "This cell runs the main MKA pipeline with learnable alpha enabled. Update paths or hyperparameters as needed for your environment before executing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4ecba033",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pipeline command:\n",
      "python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
      "\n",
      "$ python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
      "[dry-run] EXECUTE_COMMANDS is False; command not executed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pipeline_command = (\n",
    "    'python pipeline.py '\n",
    "    '--model_path \"meta-llama/Meta-Llama-3-8B\" '\n",
    "    '--num_layer 14 '\n",
    "    '--data_dir \"./data\" '\n",
    "    '--use_learnable_alpha '\n",
    "    '--alpha_training_steps 500 '\n",
    "    '--alpha_learning_rate 1e-4'\n",
    ")\n",
    "\n",
    "print(\"Pipeline command:\")\n",
    "print(pipeline_command)\n",
    "run_command(pipeline_command, cwd=PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "543f1c1b",
   "metadata": {},
   "source": [
    "## 4. Run MMLU Evaluation Command\n",
    "Evaluates the fused model on MMLU. Adjust the arguments if your evaluation script expects different paths (e.g., `--output_dir`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1c4e37d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluation command:\n",
      "python evaluate_methods.py --model_path \"./merged_weights/learnable_alpha\" --output_file \"results.json\"\n",
      "\n",
      "$ python evaluate_methods.py --model_path \"./merged_weights/learnable_alpha\" --output_file \"results.json\"\n",
      "[dry-run] EXECUTE_COMMANDS is False; command not executed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluation_command = (\n",
    "    'python evaluate_methods.py '\n",
    "    '--model_path \"./merged_weights/learnable_alpha\" '\n",
    "    '--output_file \"results.json\"'\n",
    ")\n",
    "\n",
    "print(\"Evaluation command:\")\n",
    "print(evaluation_command)\n",
    "run_command(evaluation_command, cwd=PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ebf6f842",
   "metadata": {},
   "source": [
    "## 5. Generate Result Plots\n",
    "This command should populate the `./plots` directory with the visual summaries of the evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "105d8628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Plotting command:\n",
      "python plot_results.py --results_file \"results.json\" --output_dir \"./plots\"\n",
      "\n",
      "$ python plot_results.py --results_file \"results.json\" --output_dir \"./plots\"\n",
      "[dry-run] EXECUTE_COMMANDS is False; command not executed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "plot_command = (\n",
    "    'python plot_results.py '\n",
    "    '--results_file \"results.json\" '\n",
    "    '--output_dir \"./plots\"'\n",
    ")\n",
    "\n",
    "print(\"Plotting command:\")\n",
    "print(plot_command)\n",
    "run_command(plot_command, cwd=PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5432cddd",
   "metadata": {},
   "source": [
    "## 6. Inspect Generated Artefacts and Logs\n",
    "After the commands finish, summarise the outputs to confirm artefacts were created."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "6b9622f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Inspecting: C:\\Users\\Parth\\OneDrive - iitgn.ac.in\\NLP\\Project\\MKA_improvement\\merged_weights\n",
      " - directory is empty\n",
      "\n",
      "Inspecting: C:\\Users\\Parth\\OneDrive - iitgn.ac.in\\NLP\\Project\\MKA_improvement\\results.json\n",
      " - [missing]\n",
      "\n",
      "Inspecting: C:\\Users\\Parth\\OneDrive - iitgn.ac.in\\NLP\\Project\\MKA_improvement\\plots\n",
      " - [missing]\n"
     ]
    }
   ],
   "source": [
    "from itertools import chain\n",
    "\n",
    "paths_to_check = [\n",
    "    PROJECT_ROOT / \"merged_weights\",\n",
    "    PROJECT_ROOT / \"results.json\",\n",
    "    PROJECT_ROOT / \"plots\",\n",
    "]\n",
    "\n",
    "for path in paths_to_check:\n",
    "    print(f\"\\nInspecting: {path}\")\n",
    "    if not path.exists():\n",
    "        print(\" - [missing]\")\n",
    "        continue\n",
    "    if path.is_file():\n",
    "        size_kb = path.stat().st_size / 1024\n",
    "        print(f\" - file size: {size_kb:.2f} KB\")\n",
    "        with path.open(\"r\", encoding=\"utf-8\", errors=\"ignore\") as handle:\n",
    "            snippet = ''.join(handle.readlines()[:5])\n",
    "        print(\" - preview:\\n\" + snippet)\n",
    "    else:\n",
    "        entries = list(path.iterdir())\n",
    "        if not entries:\n",
    "            print(\" - directory is empty\")\n",
    "        else:\n",
    "            for entry in entries:\n",
    "                kind = \"dir\" if entry.is_dir() else \"file\"\n",
    "                print(f\" - ({kind}) {entry.name}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd1f4259",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Alternative: MLP-Based Dynamic Merging\n",
    "\n",
    "Instead of a scalar α, use an MLP that predicts α based on input statistics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f75d0e93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MLP-based merging command:\n",
      "python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --use_mlp_merge --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
      "\n",
      "$ python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --use_mlp_merge --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
      "[dry-run] EXECUTE_COMMANDS is False; command not executed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mlp_pipeline_command = (\n",
    "    'python pipeline.py '\n",
    "    '--model_path \"meta-llama/Meta-Llama-3-8B\" '\n",
    "    '--num_layer 14 '\n",
    "    '--data_dir \"./data\" '\n",
    "    '--use_learnable_alpha '\n",
    "    '--use_mlp_merge '\n",
    "    '--alpha_training_steps 500 '\n",
    "    '--alpha_learning_rate 1e-4'\n",
    ")\n",
    "\n",
    "print(\"MLP-based merging command:\")\n",
    "print(mlp_pipeline_command)\n",
    "run_command(mlp_pipeline_command, cwd=PROJECT_ROOT)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d484e187",
   "metadata": {},
   "source": [
    "## Comparing Both Methods\n",
    "\n",
    "Run full evaluation suite including both scalar α and MLP α variants."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "62f46368",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Full evaluation suite (includes MLP):\n",
      "python evaluate_methods.py --model_path \"meta-llama/Meta-Llama-3-8B\" --data_dir \"./data\" --similarity_matrix \"similarity_matrix.pkl\" --output_dir \"./experiments\" --include_mlp\n",
      "\n",
      "$ python evaluate_methods.py --model_path \"meta-llama/Meta-Llama-3-8B\" --data_dir \"./data\" --similarity_matrix \"similarity_matrix.pkl\" --output_dir \"./experiments\" --include_mlp\n",
      "[dry-run] EXECUTE_COMMANDS is False; command not executed.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "full_eval_command = (\n",
    "    'python evaluate_methods.py '\n",
    "    '--model_path \"meta-llama/Meta-Llama-3-8B\" '\n",
    "    '--data_dir \"./data\" '\n",
    "    '--similarity_matrix \"similarity_matrix.pkl\" '\n",
    "    '--output_dir \"./experiments\" '\n",
    "    '--include_mlp'\n",
    ")\n",
    "\n",
    "print(\"Full evaluation suite (includes MLP):\")\n",
    "print(full_eval_command)\n",
    "run_command(full_eval_command, cwd=PROJECT_ROOT)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
