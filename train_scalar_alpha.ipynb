{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530afdd1",
   "metadata": {},
   "source": [
    "# Learnable Scalar Alpha Training for MKA Layer Merging\n",
    "\n",
    "This notebook demonstrates training a **scalar alpha parameter** to find optimal layer merging coefficients.\n",
    "\n",
    "**Goal**: Test whether the paper's S_lm heuristic is truly optimal by learning Œ± via gradient descent.\n",
    "\n",
    "## Method\n",
    "- Replace layer pairs with `MergeableLayer` wrappers\n",
    "- Each wrapper has a trainable scalar Œ± (logit-parameterized)\n",
    "- Train Œ± parameters on calibration data while keeping original layers frozen\n",
    "- Compare learned Œ± values with similarity-based heuristic\n",
    "\n",
    "---\n",
    "\n",
    "## üí° **Important Notes**\n",
    "- **For actual training**: Run commands directly in terminal (see commands below)\n",
    "- **This notebook**: Use for interactive analysis and visualization of results\n",
    "- **Safety feature**: `EXECUTE_COMMANDS = False` by default to prevent accidental runs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aab97",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3edd7d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "SCALAR ALPHA TRAINING - CONFIGURATION\n",
      "============================================================\n",
      "  Model: meta-llama/Meta-Llama-3-8B\n",
      "  Data directory: ./data\n",
      "  Layers to merge: 14\n",
      "  Training steps: 500\n",
      "  Learning rate: 0.0001\n",
      "  Batch size: 4\n",
      "  Calibration samples: 100\n",
      "============================================================\n",
      "‚ö†Ô∏è  EXECUTE_COMMANDS = False\n",
      "    (Commands will show as dry-run only)\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATA_DIR = \"./data\"\n",
    "NUM_LAYERS = 14  # Number of layer pairs to merge\n",
    "OUTPUT_DIR = \"./merged_weights\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ALPHA_TRAINING_STEPS = 500\n",
    "ALPHA_LEARNING_RATE = 1e-4\n",
    "CALIBRATION_BATCH_SIZE = 4\n",
    "CALIBRATION_SAMPLES = 100\n",
    "\n",
    "# Safety toggle - set to True to execute commands in this notebook\n",
    "# RECOMMENDED: Run commands directly in terminal instead\n",
    "EXECUTE_COMMANDS = False\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALAR ALPHA TRAINING - CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Layers to merge: {NUM_LAYERS}\")\n",
    "print(f\"  Training steps: {ALPHA_TRAINING_STEPS}\")\n",
    "print(f\"  Learning rate: {ALPHA_LEARNING_RATE}\")\n",
    "print(f\"  Batch size: {CALIBRATION_BATCH_SIZE}\")\n",
    "print(f\"  Calibration samples: {CALIBRATION_SAMPLES}\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"‚ö†Ô∏è  EXECUTE_COMMANDS = {EXECUTE_COMMANDS}\")\n",
    "if not EXECUTE_COMMANDS:\n",
    "    print(\"    (Commands will show as dry-run only)\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10f4c",
   "metadata": {},
   "source": [
    "## Step 1: Verify Required Files\n",
    "\n",
    "Before running training, ensure all required files exist:\n",
    "- Model checkpoint or HuggingFace model access\n",
    "- MMLU data files in `data/dev/` and `data/test/`\n",
    "- Similarity matrix (optional, for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "909933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data directory exists\n",
      "  Dev files: 57\n",
      "  Test files: 57\n",
      "‚úì Similarity matrix found: similarity_matrix.pkl\n",
      "‚úì Output directory ready: ./merged_weights\n"
     ]
    }
   ],
   "source": [
    "# Check data directory\n",
    "if os.path.exists(DATA_DIR):\n",
    "    dev_files = os.listdir(os.path.join(DATA_DIR, \"dev\")) if os.path.exists(os.path.join(DATA_DIR, \"dev\")) else []\n",
    "    test_files = os.listdir(os.path.join(DATA_DIR, \"test\")) if os.path.exists(os.path.join(DATA_DIR, \"test\")) else []\n",
    "    print(f\"‚úì Data directory exists\")\n",
    "    print(f\"  Dev files: {len(dev_files)}\")\n",
    "    print(f\"  Test files: {len(test_files)}\")\n",
    "else:\n",
    "    print(f\"‚úó Data directory not found: {DATA_DIR}\")\n",
    "\n",
    "# Check similarity matrix (optional)\n",
    "similarity_matrix_path = \"similarity_matrix.pkl\"\n",
    "if os.path.exists(similarity_matrix_path):\n",
    "    print(f\"‚úì Similarity matrix found: {similarity_matrix_path}\")\n",
    "else:\n",
    "    print(f\"‚ö†Ô∏è Similarity matrix not found (optional): {similarity_matrix_path}\")\n",
    "\n",
    "# Check output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"‚úì Output directory ready: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa49aea",
   "metadata": {},
   "source": [
    "## Step 2: Train Scalar Alpha Parameters\n",
    "\n",
    "This command will:\n",
    "1. Load the Llama-3-8B model\n",
    "2. Replace selected layer pairs with `MergeableLayer` wrappers\n",
    "3. Train scalar Œ± parameters using calibration data\n",
    "4. Save learned Œ± values to `learned_alphas.json`\n",
    "5. Fuse layers and save the merged model\n",
    "\n",
    "### üöÄ **Recommended: Run in Terminal**\n",
    "Copy and run this command directly in PowerShell:\n",
    "```powershell\n",
    "python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11cab93e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment and run this cell to execute training directly\n",
    "!python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
    "\n",
    "print(\"üëÜ Uncomment the line above and run this cell to train scalar alpha\")\n",
    "print(\"   OR use the subprocess approach in the next cell\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb3f6e64",
   "metadata": {},
   "source": [
    "## Alternative: Run Command Directly in Notebook\n",
    "\n",
    "You can also run the command directly in a notebook cell using `!`:\n",
    "- The `!` prefix executes shell commands from within the notebook\n",
    "- This is simpler than the `subprocess` approach below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5c67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "TRAINING COMMAND\n",
      "============================================================\n",
      "python pipeline.py --model_path meta-llama/Meta-Llama-3-8B --num_layer 14 --data_dir ./data --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 0.0001 --calibration_batch_size 4 --calibration_samples 100\n",
      "============================================================\n",
      "\n",
      "‚ö†Ô∏è  EXECUTE_COMMANDS is False\n",
      "   To run: Set EXECUTE_COMMANDS = True in configuration cell\n",
      "   OR better: Copy the command above and run in terminal\n"
     ]
    }
   ],
   "source": [
    "# # Build the command\n",
    "# cmd = [\n",
    "#     \"python\", \"pipeline.py\",\n",
    "#     \"--model_path\", MODEL_PATH,\n",
    "#     \"--num_layer\", str(NUM_LAYERS),\n",
    "#     \"--data_dir\", DATA_DIR,\n",
    "#     \"--use_learnable_alpha\",\n",
    "#     \"--alpha_training_steps\", str(ALPHA_TRAINING_STEPS),\n",
    "#     \"--alpha_learning_rate\", str(ALPHA_LEARNING_RATE),\n",
    "#     \"--calibration_batch_size\", str(CALIBRATION_BATCH_SIZE),\n",
    "#     \"--calibration_samples\", str(CALIBRATION_SAMPLES),\n",
    "# ]\n",
    "\n",
    "# print(\"=\" * 60)\n",
    "# print(\"TRAINING COMMAND\")\n",
    "# print(\"=\" * 60)\n",
    "# print(\" \".join(cmd))\n",
    "# print(\"=\" * 60)\n",
    "\n",
    "# if EXECUTE_COMMANDS:\n",
    "#     print(\"\\nüöÄ Executing training...\\n\")\n",
    "#     result = subprocess.run(cmd, capture_output=False, text=True)\n",
    "#     if result.returncode == 0:\n",
    "#         print(\"\\n‚úÖ Training completed successfully!\")\n",
    "#     else:\n",
    "#         print(f\"\\n‚ùå Training failed with exit code {result.returncode}\")\n",
    "# else:\n",
    "#     print(\"\\n‚ö†Ô∏è  EXECUTE_COMMANDS is False\")\n",
    "#     print(\"   To run: Set EXECUTE_COMMANDS = True in configuration cell\")\n",
    "#     print(\"   OR better: Copy the command above and run in terminal\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206c5c1",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Learned Alpha Values\n",
    "\n",
    "After training, examine the learned Œ± values and compare with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faa076",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úó Learned alphas file not found: ./merged_weights\\learned_alphas.json\n",
      "Run the training step first.\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learned_alphas_path = os.path.join(OUTPUT_DIR, \"learned_alphas.json\")\n",
    "\n",
    "if os.path.exists(learned_alphas_path):\n",
    "    with open(learned_alphas_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    learned_alphas = data.get('learned_alphas', [])\n",
    "    similarity_scores = data.get('similarity_scores', [])\n",
    "    \n",
    "    print(f\"Number of learned alphas: {len(learned_alphas)}\")\n",
    "    print(f\"Alpha statistics:\")\n",
    "    print(f\"  Mean: {np.mean(learned_alphas):.4f}\")\n",
    "    print(f\"  Std:  {np.std(learned_alphas):.4f}\")\n",
    "    print(f\"  Min:  {np.min(learned_alphas):.4f}\")\n",
    "    print(f\"  Max:  {np.max(learned_alphas):.4f}\")\n",
    "    \n",
    "    if similarity_scores:\n",
    "        print(f\"\\nSimilarity scores:\")\n",
    "        print(f\"  Mean: {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  Std:  {np.std(similarity_scores):.4f}\")\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(learned_alphas, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Learned Alpha Values')\n",
    "    plt.axvline(x=np.mean(learned_alphas), color='r', linestyle='--', label=f'Mean: {np.mean(learned_alphas):.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot alpha vs similarity\n",
    "    if similarity_scores and len(similarity_scores) == len(learned_alphas):\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(similarity_scores, learned_alphas, alpha=0.6)\n",
    "        plt.xlabel('Similarity Score')\n",
    "        plt.ylabel('Learned Alpha')\n",
    "        plt.title('Learned Alpha vs Similarity Score')\n",
    "        \n",
    "        # Add correlation\n",
    "        corr = np.corrcoef(similarity_scores, learned_alphas)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"‚úó Learned alphas file not found: {learned_alphas_path}\")\n",
    "    print(\"Run the training step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d37b2",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Merged Model\n",
    "\n",
    "Test the merged model on MMLU benchmark to measure accuracy.\n",
    "\n",
    "### üéØ **For Comprehensive Comparison**\n",
    "Run this command in terminal to compare all 5 methods:\n",
    "```powershell\n",
    "python evaluate_methods.py --model_path \"meta-llama/Meta-Llama-3-8B\" --data_dir \"./data\" --similarity_matrix \"similarity_matrix.pkl\" --output_dir \"./experiments\"\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f82ce5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "============================================================\n",
      "EVALUATION COMMAND\n",
      "============================================================\n",
      "python evaluate_methods.py --model_path meta-llama/Meta-Llama-3-8B --data_dir ./data --output_dir ./experiments\n",
      "============================================================\n",
      "\n",
      "üìÅ Merged model saved in: ./merged_weights\n",
      "\n",
      "üí° This evaluates the scalar alpha method.\n",
      "   To also include MLP comparison, add: --include_mlp\n"
     ]
    }
   ],
   "source": [
    "# Evaluation command\n",
    "print(\"=\" * 60)\n",
    "print(\"EVALUATION COMMAND\")\n",
    "print(\"=\" * 60)\n",
    "eval_cmd = f\"python evaluate_methods.py --model_path {MODEL_PATH} --data_dir {DATA_DIR} --output_dir ./experiments\"\n",
    "print(eval_cmd)\n",
    "print(\"=\" * 60)\n",
    "print(f\"\\nüìÅ Merged model saved in: {OUTPUT_DIR}\")\n",
    "print(\"\\nüí° This evaluates the scalar alpha method.\")\n",
    "print(\"   To also include MLP comparison, add: --include_mlp\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae34a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ‚úÖ Training scalar Œ± parameters for layer merging\n",
    "2. ‚úÖ Analyzing learned Œ± values and their relationship to similarity scores\n",
    "3. ‚úÖ Saving the merged model for evaluation\n",
    "\n",
    "**Key Questions to Answer:**\n",
    "- Do learned Œ± values differ significantly from similarity-based heuristic?\n",
    "- Does learning Œ± improve model accuracy compared to fixed Œ±?\n",
    "- What patterns emerge in the learned Œ± distribution?\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with MLP-based merging (see `train_mlp_alpha.ipynb`)\n",
    "- Run full evaluation suite to compare all methods\n",
    "- Analyze per-layer Œ± patterns and their correlation with layer properties\n",
    "\n",
    "---\n",
    "\n",
    "## üîÑ **Quick Reference: All Commands**\n",
    "\n",
    "### Scalar Alpha Training (This Notebook):\n",
    "```powershell\n",
    "python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
    "```\n",
    "\n",
    "### MLP Alpha Training (See Other Notebook):\n",
    "```powershell\n",
    "python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 14 --data_dir \"./data\" --use_learnable_alpha --use_mlp_merge --alpha_training_steps 500 --alpha_learning_rate 1e-4\n",
    "```\n",
    "\n",
    "### Full Evaluation (Compare All 5 Methods):\n",
    "```powershell\n",
    "python evaluate_methods.py --model_path \"meta-llama/Meta-Llama-3-8B\" --data_dir \"./data\" --similarity_matrix \"similarity_matrix.pkl\" --output_dir \"./experiments\" --include_mlp\n",
    "```"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
