{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530afdd1",
   "metadata": {},
   "source": [
    "# Learnable Scalar Alpha Training for MKA Layer Merging\n",
    "\n",
    "This notebook compares **baseline MKA** (similarity-based) vs **learnable scalar alpha** approach.\n",
    "\n",
    "## Workflow:\n",
    "1. **Run Baseline**: Original MKA with similarity-based merging (num_layer=13)\n",
    "2. **Train Alpha**: Learn optimal Œ± parameters via gradient descent\n",
    "3. **Evaluate**: Test both on MMLU and compare accuracy\n",
    "\n",
    "## Goal:\n",
    "Test whether learning Œ± via gradient descent improves over the paper's S_lm heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aab97",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd7d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to HuggingFace\n",
      "============================================================\n",
      "SCALAR ALPHA EXPERIMENT - CONFIGURATION\n",
      "============================================================\n",
      "  Model: meta-llama/Meta-Llama-3-8B\n",
      "  Layers to merge: 13\n",
      "  Training steps: 500\n",
      "  Learning rate: 0.0001\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# HuggingFace Authentication\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úì Logged in to HuggingFace\")\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATA_DIR = \"./data\"\n",
    "NUM_LAYERS = 13  # Must match your baseline evaluation\n",
    "OUTPUT_DIR_BASELINE = \"./output_baseline\"\n",
    "OUTPUT_DIR_LEARNED = \"./output_learned\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ALPHA_TRAINING_STEPS = 500\n",
    "ALPHA_LEARNING_RATE = 1e-4\n",
    "CALIBRATION_BATCH_SIZE = 4\n",
    "CALIBRATION_SAMPLES = 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALAR ALPHA EXPERIMENT - CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Layers to merge: {NUM_LAYERS}\")\n",
    "print(f\"  Training steps: {ALPHA_TRAINING_STEPS}\")\n",
    "print(f\"  Learning rate: {ALPHA_LEARNING_RATE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10f4c",
   "metadata": {},
   "source": [
    "## Step 1: Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd054b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Download MMLU dataset (only need to run once)\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    print(\"üì• Downloading MMLU dataset...\")\n",
    "    # Clone the official MMLU repository\n",
    "    !git clone https://github.com/hendrycks/test.git mmlu_download\n",
    "    \n",
    "    # Move the data folder\n",
    "    !mv mmlu_download/data ./data\n",
    "    \n",
    "    # Clean up\n",
    "    !rm -rf mmlu_download\n",
    "    \n",
    "    # Verify structure\n",
    "    if os.path.exists(\"./data/dev\") and os.path.exists(\"./data/test\"):\n",
    "        print(\"‚úÖ MMLU dataset downloaded successfully!\")\n",
    "        dev_count = len([f for f in os.listdir(\"./data/dev\") if f.endswith(\"_dev.csv\")])\n",
    "        test_count = len([f for f in os.listdir(\"./data/test\") if f.endswith(\"_test.csv\")])\n",
    "        print(f\"   Dev files: {dev_count}, Test files: {test_count}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Download completed but structure looks wrong\")\n",
    "else:\n",
    "    print(\"‚úÖ Data directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311b2b",
   "metadata": {},
   "source": [
    "## Step 0: Download MMLU Dataset (Lightning AI Setup)\n",
    "\n",
    "**First time only:** Download MMLU dataset from the official source."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data directory exists: 57 dev files, 57 test files\n"
     ]
    }
   ],
   "source": [
    "# Check data directory\n",
    "if os.path.exists(DATA_DIR):\n",
    "    dev_files = os.listdir(os.path.join(DATA_DIR, \"dev\")) if os.path.exists(os.path.join(DATA_DIR, \"dev\")) else []\n",
    "    test_files = os.listdir(os.path.join(DATA_DIR, \"test\")) if os.path.exists(os.path.join(DATA_DIR, \"test\")) else []\n",
    "    print(f\"‚úì Data directory exists: {len(dev_files)} dev files, {len(test_files)} test files\")\n",
    "else:\n",
    "    print(f\"‚úó Data directory not found: {DATA_DIR}\")\n",
    "    print(\"  Make sure MMLU data is in ./data/dev/ and ./data/test/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0f2e989",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train learnable alpha and evaluate on MMLU\n",
    "!python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 13 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80601e9c",
   "metadata": {},
   "source": [
    "## Step 2B: Train Learnable Scalar Alpha\n",
    "\n",
    "Train Œ± parameters on calibration data, then evaluate on MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS if you already have baseline results for num_layer=13\n",
    "# Uncomment to run baseline:\n",
    "# !python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 13 --data_dir \"./data\"\n",
    "\n",
    "print(\"‚ö†Ô∏è Skipping baseline - using existing results for num_layer=13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ab66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your actual baseline accuracy for num_layer=13\n",
    "baseline_accuracy = 0.0  # <-- UPDATE THIS WITH YOUR BASELINE RESULT\n",
    "\n",
    "# Load learned alpha MMLU results\n",
    "results_path = \"./output/Meta-Llama-3-8B/fused_13_layers/iteration/fusion_info/mmlu_results.json\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MMLU ACCURACY COMPARISON (num_layer=13)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    try:\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            learned_accuracy = results.get('average_accuracy', 0.0)\n",
    "        \n",
    "        print(f\"Baseline (Similarity-based):  {baseline_accuracy:.4f}\")\n",
    "        print(f\"Learned Scalar Alpha:         {learned_accuracy:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        improvement = learned_accuracy - baseline_accuracy\n",
    "        improvement_pct = (improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else 0\n",
    "        \n",
    "        print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Bar chart\n",
    "        methods = ['Baseline\\n(Similarity)', 'Learned\\nScalar Œ±']\n",
    "        accuracies = [baseline_accuracy, learned_accuracy]\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        \n",
    "        bars = ax1.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "        ax1.set_ylabel('MMLU Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Baseline vs Learned Alpha', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylim([min(accuracies) * 0.95 if min(accuracies) > 0 else 0, max(accuracies) * 1.05])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.4f}',\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Improvement chart\n",
    "        ax2.bar(['Improvement'], [improvement], color='green' if improvement > 0 else 'red', \n",
    "               alpha=0.8, edgecolor='black', linewidth=2)\n",
    "        ax2.set_ylabel('Accuracy Difference', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Performance Gain', fontsize=14, fontweight='bold')\n",
    "        ax2.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "        ax2.text(0, improvement, f'{improvement:+.4f}\\n({improvement_pct:+.2f}%)', \n",
    "                ha='center', va='bottom' if improvement > 0 else 'top', \n",
    "                fontsize=12, fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print per-subject results if available\n",
    "        if 'per_subject' in results:\n",
    "            print(\"\\nTop 5 subjects by accuracy:\")\n",
    "            subject_accs = [(k, v) for k, v in results['per_subject'].items()]\n",
    "            subject_accs.sort(key=lambda x: x[1], reverse=True)\n",
    "            for subject, acc in subject_accs[:5]:\n",
    "                print(f\"  {subject:40s}: {acc:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è Error loading results: {e}\")\n",
    "else:\n",
    "    print(f\"‚úó Results not found: {results_path}\")\n",
    "    print(\"  Training must complete first.\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065ccdc",
   "metadata": {},
   "source": [
    "## Step 4: Compare MMLU Accuracy - Baseline vs Learned\n",
    "\n",
    "**Important:** Update `baseline_accuracy` with your actual baseline result for num_layer=13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load learned alphas\n",
    "learned_alphas_path = \"./output/Meta-Llama-3-8B/fused_13_layers/iteration/merged_weights/learned_alphas.json\"\n",
    "\n",
    "if os.path.exists(learned_alphas_path):\n",
    "    with open(learned_alphas_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    learned_alphas = data.get('learned_alphas', [])\n",
    "    similarity_scores = data.get('similarity_scores', [])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LEARNED ALPHA STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Number of layers: {len(learned_alphas)}\")\n",
    "    print(f\"  Mean Œ±: {np.mean(learned_alphas):.4f}\")\n",
    "    print(f\"  Std Œ±:  {np.std(learned_alphas):.4f}\")\n",
    "    print(f\"  Min Œ±:  {np.min(learned_alphas):.4f}\")\n",
    "    print(f\"  Max Œ±:  {np.max(learned_alphas):.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize alpha distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(learned_alphas, bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    plt.axvline(np.mean(learned_alphas), color='r', linestyle='--', label=f'Mean: {np.mean(learned_alphas):.3f}')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Learned Œ±')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs layer index\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(len(learned_alphas)), learned_alphas, marker='o', linestyle='-', color='darkgreen')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Learned Œ±')\n",
    "    plt.title('Learned Œ± Across Layers')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs Similarity\n",
    "    if similarity_scores and len(similarity_scores) == len(learned_alphas):\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(similarity_scores, learned_alphas, alpha=0.6, s=100, color='coral')\n",
    "        plt.xlabel('Similarity Score (S_lm)')\n",
    "        plt.ylabel('Learned Œ±')\n",
    "        plt.title('Learned Œ± vs Similarity')\n",
    "        corr = np.corrcoef(similarity_scores, learned_alphas)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Analysis complete! Check the plots above.\")\n",
    "else:\n",
    "    print(f\"‚úó Learned alphas not found: {learned_alphas_path}\")\n",
    "    print(\"  Training may still be running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac19a5",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Results\n",
    "\n",
    "After training completes, analyze learned alpha values and MMLU accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa120e7b",
   "metadata": {},
   "source": [
    "## Step 2A: Run Baseline (Original MKA - No Alpha Training)\n",
    "\n",
    "**Note:** You mentioned you already have baseline results for num_layer=13, so you can skip this step.\n",
    "\n",
    "If you need to run it again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae34a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared:\n",
    "1. **Baseline MKA** (similarity-based heuristic S_lm)\n",
    "2. **Learned Scalar Œ±** (trained via gradient descent)\n",
    "\n",
    "**Key Findings:**\n",
    "- Learned Œ± values may differ from similarity scores\n",
    "- Check correlation between learned Œ± and S_lm\n",
    "- Compare MMLU accuracy to see if learning improves performance\n",
    "\n",
    "**Next Steps:**\n",
    "- Try MLP-based dynamic merging (see `train_mlp_alpha.ipynb`)\n",
    "- Run full comparison with `evaluate_methods.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
