{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530afdd1",
   "metadata": {},
   "source": [
    "# Learnable Scalar Alpha Training for MKA Layer Merging\n",
    "\n",
    "This notebook demonstrates training a **scalar alpha parameter** to find optimal layer merging coefficients.\n",
    "\n",
    "**Goal**: Test whether the paper's S_lm heuristic is truly optimal by learning α via gradient descent.\n",
    "\n",
    "## Method\n",
    "- Replace layer pairs with `MergeableLayer` wrappers\n",
    "- Each wrapper has a trainable scalar α (logit-parameterized)\n",
    "- Train α parameters on calibration data while keeping original layers frozen\n",
    "- Compare learned α values with similarity-based heuristic"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aab97",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3edd7d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATA_DIR = \"./data\"\n",
    "NUM_LAYERS = 14  # Number of layer pairs to merge\n",
    "OUTPUT_DIR = \"./merged_weights\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ALPHA_TRAINING_STEPS = 500\n",
    "ALPHA_LEARNING_RATE = 1e-4\n",
    "CALIBRATION_BATCH_SIZE = 4\n",
    "CALIBRATION_SAMPLES = 100\n",
    "\n",
    "# Safety toggle - set to True to execute commands\n",
    "EXECUTE_COMMANDS = False\n",
    "\n",
    "print(\"Configuration:\")\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Data directory: {DATA_DIR}\")\n",
    "print(f\"  Layers to merge: {NUM_LAYERS}\")\n",
    "print(f\"  Training steps: {ALPHA_TRAINING_STEPS}\")\n",
    "print(f\"  Learning rate: {ALPHA_LEARNING_RATE}\")\n",
    "print(f\"\\n⚠️ EXECUTE_COMMANDS = {EXECUTE_COMMANDS}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10f4c",
   "metadata": {},
   "source": [
    "## Step 1: Verify Required Files\n",
    "\n",
    "Before running training, ensure all required files exist:\n",
    "- Model checkpoint or HuggingFace model access\n",
    "- MMLU data files in `data/dev/` and `data/test/`\n",
    "- Similarity matrix (optional, for comparison)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909933fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check data directory\n",
    "if os.path.exists(DATA_DIR):\n",
    "    dev_files = os.listdir(os.path.join(DATA_DIR, \"dev\")) if os.path.exists(os.path.join(DATA_DIR, \"dev\")) else []\n",
    "    test_files = os.listdir(os.path.join(DATA_DIR, \"test\")) if os.path.exists(os.path.join(DATA_DIR, \"test\")) else []\n",
    "    print(f\"✓ Data directory exists\")\n",
    "    print(f\"  Dev files: {len(dev_files)}\")\n",
    "    print(f\"  Test files: {len(test_files)}\")\n",
    "else:\n",
    "    print(f\"✗ Data directory not found: {DATA_DIR}\")\n",
    "\n",
    "# Check similarity matrix (optional)\n",
    "similarity_matrix_path = \"similarity_matrix.pkl\"\n",
    "if os.path.exists(similarity_matrix_path):\n",
    "    print(f\"✓ Similarity matrix found: {similarity_matrix_path}\")\n",
    "else:\n",
    "    print(f\"⚠️ Similarity matrix not found (optional): {similarity_matrix_path}\")\n",
    "\n",
    "# Check output directory\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "print(f\"✓ Output directory ready: {OUTPUT_DIR}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7aa49aea",
   "metadata": {},
   "source": [
    "## Step 2: Train Scalar Alpha Parameters\n",
    "\n",
    "This command will:\n",
    "1. Load the Llama-3-8B model\n",
    "2. Replace selected layer pairs with `MergeableLayer` wrappers\n",
    "3. Train scalar α parameters using calibration data\n",
    "4. Save learned α values to `learned_alphas.json`\n",
    "5. Fuse layers and save the merged model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "268c5c67",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the command\n",
    "cmd = [\n",
    "    \"python\", \"pipeline.py\",\n",
    "    \"--model_path\", MODEL_PATH,\n",
    "    \"--num_layer\", str(NUM_LAYERS),\n",
    "    \"--data_dir\", DATA_DIR,\n",
    "    \"--use_learnable_alpha\",\n",
    "    \"--alpha_training_steps\", str(ALPHA_TRAINING_STEPS),\n",
    "    \"--alpha_learning_rate\", str(ALPHA_LEARNING_RATE),\n",
    "    \"--calibration_batch_size\", str(CALIBRATION_BATCH_SIZE),\n",
    "    \"--calibration_samples\", str(CALIBRATION_SAMPLES),\n",
    "]\n",
    "\n",
    "print(\"Command to execute:\")\n",
    "print(\" \".join(cmd))\n",
    "print()\n",
    "\n",
    "if EXECUTE_COMMANDS:\n",
    "    print(\"Executing training...\\n\")\n",
    "    result = subprocess.run(cmd, capture_output=False, text=True)\n",
    "    if result.returncode == 0:\n",
    "        print(\"\\n✓ Training completed successfully!\")\n",
    "    else:\n",
    "        print(f\"\\n✗ Training failed with exit code {result.returncode}\")\n",
    "else:\n",
    "    print(\"⚠️ EXECUTE_COMMANDS is False. Set it to True to run training.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b206c5c1",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Learned Alpha Values\n",
    "\n",
    "After training, examine the learned α values and compare with similarity scores."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17faa076",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "learned_alphas_path = os.path.join(OUTPUT_DIR, \"learned_alphas.json\")\n",
    "\n",
    "if os.path.exists(learned_alphas_path):\n",
    "    with open(learned_alphas_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    learned_alphas = data.get('learned_alphas', [])\n",
    "    similarity_scores = data.get('similarity_scores', [])\n",
    "    \n",
    "    print(f\"Number of learned alphas: {len(learned_alphas)}\")\n",
    "    print(f\"Alpha statistics:\")\n",
    "    print(f\"  Mean: {np.mean(learned_alphas):.4f}\")\n",
    "    print(f\"  Std:  {np.std(learned_alphas):.4f}\")\n",
    "    print(f\"  Min:  {np.min(learned_alphas):.4f}\")\n",
    "    print(f\"  Max:  {np.max(learned_alphas):.4f}\")\n",
    "    \n",
    "    if similarity_scores:\n",
    "        print(f\"\\nSimilarity scores:\")\n",
    "        print(f\"  Mean: {np.mean(similarity_scores):.4f}\")\n",
    "        print(f\"  Std:  {np.std(similarity_scores):.4f}\")\n",
    "    \n",
    "    # Plot histogram\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.hist(learned_alphas, bins=20, edgecolor='black')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Learned Alpha Values')\n",
    "    plt.axvline(x=np.mean(learned_alphas), color='r', linestyle='--', label=f'Mean: {np.mean(learned_alphas):.3f}')\n",
    "    plt.legend()\n",
    "    \n",
    "    # Plot alpha vs similarity\n",
    "    if similarity_scores and len(similarity_scores) == len(learned_alphas):\n",
    "        plt.subplot(1, 2, 2)\n",
    "        plt.scatter(similarity_scores, learned_alphas, alpha=0.6)\n",
    "        plt.xlabel('Similarity Score')\n",
    "        plt.ylabel('Learned Alpha')\n",
    "        plt.title('Learned Alpha vs Similarity Score')\n",
    "        \n",
    "        # Add correlation\n",
    "        corr = np.corrcoef(similarity_scores, learned_alphas)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', \n",
    "                transform=plt.gca().transAxes, verticalalignment='top')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "else:\n",
    "    print(f\"✗ Learned alphas file not found: {learned_alphas_path}\")\n",
    "    print(\"Run the training step first.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa7d37b2",
   "metadata": {},
   "source": [
    "## Step 4: Evaluate Merged Model\n",
    "\n",
    "Test the merged model on MMLU benchmark to measure accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0f82ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This would require running the evaluation separately\n",
    "# The merged model is saved in the output directory\n",
    "\n",
    "print(f\"Merged model saved in: {OUTPUT_DIR}\")\n",
    "print(\"\\nTo evaluate the model, you can:\")\n",
    "print(\"1. Use the evaluate_methods.py script for comprehensive comparison\")\n",
    "print(\"2. Load the merged model and test on your own datasets\")\n",
    "print(\"\\nExample evaluation command:\")\n",
    "eval_cmd = f\"python evaluate_methods.py --model_path {MODEL_PATH} --data_dir {DATA_DIR} --output_dir ./experiments\"\n",
    "print(eval_cmd)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae34a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook demonstrated:\n",
    "1. ✅ Training scalar α parameters for layer merging\n",
    "2. ✅ Analyzing learned α values and their relationship to similarity scores\n",
    "3. ✅ Saving the merged model for evaluation\n",
    "\n",
    "**Key Questions to Answer:**\n",
    "- Do learned α values differ significantly from similarity-based heuristic?\n",
    "- Does learning α improve model accuracy compared to fixed α?\n",
    "- What patterns emerge in the learned α distribution?\n",
    "\n",
    "**Next Steps:**\n",
    "- Compare with MLP-based merging (see `train_mlp_alpha.ipynb`)\n",
    "- Run full evaluation suite to compare all methods\n",
    "- Analyze per-layer α patterns and their correlation with layer properties"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
