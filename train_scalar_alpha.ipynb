{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530afdd1",
   "metadata": {},
   "source": [
    "# Learnable Scalar Alpha Training for MKA Layer Merging\n",
    "\n",
    "This notebook compares **baseline MKA** (similarity-based) vs **learnable scalar alpha** approach.\n",
    "\n",
    "## Workflow:\n",
    "1. **Run Baseline**: Original MKA with similarity-based merging (num_layer=13)\n",
    "2. **Train Alpha**: Learn optimal α parameters via gradient descent\n",
    "3. **Evaluate**: Test both on MMLU and compare accuracy\n",
    "\n",
    "## Goal:\n",
    "Test whether learning α via gradient descent improves over the paper's S_lm heuristic."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aab97",
   "metadata": {},
   "source": [
    "## Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3edd7d45",
   "metadata": {},
   "outputs": [
    {
     "ename": "HTTPError",
     "evalue": "Invalid user token. The token stored is invalid. Please run `huggingface-cli login` to update it.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:409\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    408\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m--> 409\u001b[0m     \u001b[43mresponse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    410\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\requests\\models.py:1024\u001b[0m, in \u001b[0;36mResponse.raise_for_status\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1023\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[1;32m-> 1024\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[1;31mHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m                            Traceback (most recent call last)",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\hf_api.py:1664\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   1663\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1664\u001b[0m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_http.py:481\u001b[0m, in \u001b[0;36mhf_raise_for_status\u001b[1;34m(response, endpoint_name)\u001b[0m\n\u001b[0;32m    479\u001b[0m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[0;32m    480\u001b[0m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[1;32m--> 481\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[1;31mHfHubHTTPError\u001b[0m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/whoami-v2 (Request ID: Root=1-69055869-7752db251b5b190a1919465c;896ac23d-7c52-4988-afc0-6cebe45e15c5)\n\nUser Access Token \"nlp\" is expired",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[1;31mHTTPError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[9], line 10\u001b[0m\n\u001b[0;32m      7\u001b[0m HF_TOKEN \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhf_AYCbZBkGqmozPjkfIvhMVdqIVMxrGJjXjq\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      9\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m HF_TOKEN:\n\u001b[1;32m---> 10\u001b[0m     \u001b[43mlogin\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mHF_TOKEN\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     11\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m✓ Logged in to HuggingFace\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_deprecation.py:101\u001b[0m, in \u001b[0;36m_deprecate_arguments.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     99\u001b[0m         message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m+\u001b[39m custom_message\n\u001b[0;32m    100\u001b[0m     warnings\u001b[38;5;241m.\u001b[39mwarn(message, \u001b[38;5;167;01mFutureWarning\u001b[39;00m)\n\u001b[1;32m--> 101\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_deprecation.py:31\u001b[0m, in \u001b[0;36m_deprecate_positional_args.<locals>._inner_deprecate_positional_args.<locals>.inner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     29\u001b[0m extra_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(args) \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mlen\u001b[39m(all_args)\n\u001b[0;32m     30\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m extra_args \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m---> 31\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m f(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m     32\u001b[0m \u001b[38;5;66;03m# extra_args > 0\u001b[39;00m\n\u001b[0;32m     33\u001b[0m args_msg \u001b[38;5;241m=\u001b[39m [\n\u001b[0;32m     34\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arg, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00marg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     35\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m name, arg \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(kwonly_args[:extra_args], args[\u001b[38;5;241m-\u001b[39mextra_args:])\n\u001b[0;32m     36\u001b[0m ]\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\_login.py:126\u001b[0m, in \u001b[0;36mlogin\u001b[1;34m(token, add_to_git_credential, new_session, write_permission)\u001b[0m\n\u001b[0;32m    119\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m add_to_git_credential:\n\u001b[0;32m    120\u001b[0m         logger\u001b[38;5;241m.\u001b[39minfo(\n\u001b[0;32m    121\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mThe token has not been saved to the git credentials helper. Pass \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    122\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`add_to_git_credential=True` in this function directly or \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    123\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`--add-to-git-credential` if using via `huggingface-cli` if \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    124\u001b[0m             \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myou want to set the git credential as well.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    125\u001b[0m         )\n\u001b[1;32m--> 126\u001b[0m     \u001b[43m_login\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43madd_to_git_credential\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43madd_to_git_credential\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    127\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m is_notebook():\n\u001b[0;32m    128\u001b[0m     notebook_login(new_session\u001b[38;5;241m=\u001b[39mnew_session)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\_login.py:404\u001b[0m, in \u001b[0;36m_login\u001b[1;34m(token, add_to_git_credential)\u001b[0m\n\u001b[0;32m    401\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m token\u001b[38;5;241m.\u001b[39mstartswith(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mapi_org\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    402\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYou must use your personal account token, not an organization token.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 404\u001b[0m token_info \u001b[38;5;241m=\u001b[39m \u001b[43mwhoami\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    405\u001b[0m permission \u001b[38;5;241m=\u001b[39m token_info[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mauth\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessToken\u001b[39m\u001b[38;5;124m\"\u001b[39m][\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m    406\u001b[0m logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mToken is valid (permission: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mpermission\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m).\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\utils\\_validators.py:114\u001b[0m, in \u001b[0;36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    111\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[0;32m    112\u001b[0m     kwargs \u001b[38;5;241m=\u001b[39m smoothly_deprecate_use_auth_token(fn_name\u001b[38;5;241m=\u001b[39mfn\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, has_token\u001b[38;5;241m=\u001b[39mhas_token, kwargs\u001b[38;5;241m=\u001b[39mkwargs)\n\u001b[1;32m--> 114\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fn(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python310\\site-packages\\huggingface_hub\\hf_api.py:1677\u001b[0m, in \u001b[0;36mHfApi.whoami\u001b[1;34m(self, token)\u001b[0m\n\u001b[0;32m   1675\u001b[0m     \u001b[38;5;28;01melif\u001b[39;00m effective_token \u001b[38;5;241m==\u001b[39m _get_token_from_file():\n\u001b[0;32m   1676\u001b[0m         error_message \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m The token stored is invalid. Please run `huggingface-cli login` to update it.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1677\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(error_message, request\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mrequest, response\u001b[38;5;241m=\u001b[39me\u001b[38;5;241m.\u001b[39mresponse) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n\u001b[0;32m   1678\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m r\u001b[38;5;241m.\u001b[39mjson()\n",
      "\u001b[1;31mHTTPError\u001b[0m: Invalid user token. The token stored is invalid. Please run `huggingface-cli login` to update it."
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# HuggingFace Authentication\n",
    "HF_TOKEN = \"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\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"✓ Logged in to HuggingFace\")\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATA_DIR = \"./data\"\n",
    "NUM_LAYERS = 13  # Must match your baseline evaluation\n",
    "OUTPUT_DIR_BASELINE = \"./output_baseline\"\n",
    "OUTPUT_DIR_LEARNED = \"./output_learned\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ALPHA_TRAINING_STEPS = 500\n",
    "ALPHA_LEARNING_RATE = 1e-4\n",
    "CALIBRATION_BATCH_SIZE = 4\n",
    "CALIBRATION_SAMPLES = 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALAR ALPHA EXPERIMENT - CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Layers to merge: {NUM_LAYERS}\")\n",
    "print(f\"  Training steps: {ALPHA_TRAINING_STEPS}\")\n",
    "print(f\"  Learning rate: {ALPHA_LEARNING_RATE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10f4c",
   "metadata": {},
   "source": [
    "## Step 1: Verify Data Files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "909933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✓ Data directory exists: 57 dev files, 57 test files\n",
      "✓ Output directories ready\n"
     ]
    }
   ],
   "source": [
    "# Check data directory\n",
    "if os.path.exists(DATA_DIR):\n",
    "    dev_files = os.listdir(os.path.join(DATA_DIR, \"dev\")) if os.path.exists(os.path.join(DATA_DIR, \"dev\")) else []\n",
    "    test_files = os.listdir(os.path.join(DATA_DIR, \"test\")) if os.path.exists(os.path.join(DATA_DIR, \"test\")) else []\n",
    "    print(f\"✓ Data directory exists: {len(dev_files)} dev files, {len(test_files)} test files\")\n",
    "else:\n",
    "    print(f\"✗ Data directory not found: {DATA_DIR}\")\n",
    "    \n",
    "os.makedirs(OUTPUT_DIR_BASELINE, exist_ok=True)\n",
    "os.makedirs(OUTPUT_DIR_LEARNED, exist_ok=True)\n",
    "print(f\"✓ Output directories ready\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c0f2e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Train learnable alpha and evaluate on MMLU\n",
    "!python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 13 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80601e9c",
   "metadata": {},
   "source": [
    "## Step 2B: Train Learnable Scalar Alpha\n",
    "\n",
    "Train α parameters on calibration data, then evaluate on MMLU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc49dbc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# SKIP THIS if you already have baseline results for num_layer=13\n",
    "# Uncomment to run baseline:\n",
    "# !python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 13 --data_dir \"./data\"\n",
    "\n",
    "print(\"⚠️ Skipping baseline - using existing results for num_layer=13\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c38ab66a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: Replace with your actual baseline accuracy for num_layer=13\n",
    "baseline_accuracy = 0.0  # <-- UPDATE THIS WITH YOUR BASELINE RESULT\n",
    "\n",
    "# Load learned alpha MMLU results\n",
    "results_path = \"./output/Meta-Llama-3-8B/fused_13_layers/iteration/fusion_info/mmlu_results.json\"\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"MMLU ACCURACY COMPARISON (num_layer=13)\")\n",
    "print(\"=\" * 60)\n",
    "\n",
    "if os.path.exists(results_path):\n",
    "    try:\n",
    "        with open(results_path, 'r') as f:\n",
    "            results = json.load(f)\n",
    "            learned_accuracy = results.get('average_accuracy', 0.0)\n",
    "        \n",
    "        print(f\"Baseline (Similarity-based):  {baseline_accuracy:.4f}\")\n",
    "        print(f\"Learned Scalar Alpha:         {learned_accuracy:.4f}\")\n",
    "        print(\"-\" * 60)\n",
    "        \n",
    "        improvement = learned_accuracy - baseline_accuracy\n",
    "        improvement_pct = (improvement / baseline_accuracy * 100) if baseline_accuracy > 0 else 0\n",
    "        \n",
    "        print(f\"Improvement: {improvement:+.4f} ({improvement_pct:+.2f}%)\")\n",
    "        print(\"=\" * 60)\n",
    "        \n",
    "        # Visualization\n",
    "        fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(12, 5))\n",
    "        \n",
    "        # Bar chart\n",
    "        methods = ['Baseline\\n(Similarity)', 'Learned\\nScalar α']\n",
    "        accuracies = [baseline_accuracy, learned_accuracy]\n",
    "        colors = ['#3498db', '#e74c3c']\n",
    "        \n",
    "        bars = ax1.bar(methods, accuracies, color=colors, alpha=0.8, edgecolor='black', linewidth=2)\n",
    "        ax1.set_ylabel('MMLU Accuracy', fontsize=12, fontweight='bold')\n",
    "        ax1.set_title('Baseline vs Learned Alpha', fontsize=14, fontweight='bold')\n",
    "        ax1.set_ylim([min(accuracies) * 0.95 if min(accuracies) > 0 else 0, max(accuracies) * 1.05])\n",
    "        \n",
    "        # Add value labels\n",
    "        for bar, acc in zip(bars, accuracies):\n",
    "            height = bar.get_height()\n",
    "            ax1.text(bar.get_x() + bar.get_width()/2., height,\n",
    "                    f'{acc:.4f}',\n",
    "                    ha='center', va='bottom', fontsize=12, fontweight='bold')\n",
    "        \n",
    "        ax1.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        # Improvement chart\n",
    "        ax2.bar(['Improvement'], [improvement], color='green' if improvement > 0 else 'red', \n",
    "               alpha=0.8, edgecolor='black', linewidth=2)\n",
    "        ax2.set_ylabel('Accuracy Difference', fontsize=12, fontweight='bold')\n",
    "        ax2.set_title('Performance Gain', fontsize=14, fontweight='bold')\n",
    "        ax2.axhline(0, color='black', linestyle='--', linewidth=1)\n",
    "        ax2.text(0, improvement, f'{improvement:+.4f}\\n({improvement_pct:+.2f}%)', \n",
    "                ha='center', va='bottom' if improvement > 0 else 'top', \n",
    "                fontsize=12, fontweight='bold')\n",
    "        ax2.grid(axis='y', alpha=0.3)\n",
    "        \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "        \n",
    "        # Print per-subject results if available\n",
    "        if 'per_subject' in results:\n",
    "            print(\"\\nTop 5 subjects by accuracy:\")\n",
    "            subject_accs = [(k, v) for k, v in results['per_subject'].items()]\n",
    "            subject_accs.sort(key=lambda x: x[1], reverse=True)\n",
    "            for subject, acc in subject_accs[:5]:\n",
    "                print(f\"  {subject:40s}: {acc:.4f}\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"⚠️ Error loading results: {e}\")\n",
    "else:\n",
    "    print(f\"✗ Results not found: {results_path}\")\n",
    "    print(\"  Training must complete first.\")\n",
    "    \n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3065ccdc",
   "metadata": {},
   "source": [
    "## Step 4: Compare MMLU Accuracy - Baseline vs Learned\n",
    "\n",
    "**Important:** Update `baseline_accuracy` with your actual baseline result for num_layer=13."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load learned alphas\n",
    "learned_alphas_path = \"./output/Meta-Llama-3-8B/fused_13_layers/iteration/merged_weights/learned_alphas.json\"\n",
    "\n",
    "if os.path.exists(learned_alphas_path):\n",
    "    with open(learned_alphas_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    learned_alphas = data.get('learned_alphas', [])\n",
    "    similarity_scores = data.get('similarity_scores', [])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LEARNED ALPHA STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Number of layers: {len(learned_alphas)}\")\n",
    "    print(f\"  Mean α: {np.mean(learned_alphas):.4f}\")\n",
    "    print(f\"  Std α:  {np.std(learned_alphas):.4f}\")\n",
    "    print(f\"  Min α:  {np.min(learned_alphas):.4f}\")\n",
    "    print(f\"  Max α:  {np.max(learned_alphas):.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize alpha distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(learned_alphas, bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    plt.axvline(np.mean(learned_alphas), color='r', linestyle='--', label=f'Mean: {np.mean(learned_alphas):.3f}')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Learned α')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs layer index\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(len(learned_alphas)), learned_alphas, marker='o', linestyle='-', color='darkgreen')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Learned α')\n",
    "    plt.title('Learned α Across Layers')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs Similarity\n",
    "    if similarity_scores and len(similarity_scores) == len(learned_alphas):\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(similarity_scores, learned_alphas, alpha=0.6, s=100, color='coral')\n",
    "        plt.xlabel('Similarity Score (S_lm)')\n",
    "        plt.ylabel('Learned α')\n",
    "        plt.title('Learned α vs Similarity')\n",
    "        corr = np.corrcoef(similarity_scores, learned_alphas)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n✓ Analysis complete! Check the plots above.\")\n",
    "else:\n",
    "    print(f\"✗ Learned alphas not found: {learned_alphas_path}\")\n",
    "    print(\"  Training may still be running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac19a5",
   "metadata": {},
   "source": [
    "## Step 3: Analyze Results\n",
    "\n",
    "After training completes, analyze learned alpha values and MMLU accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa120e7b",
   "metadata": {},
   "source": [
    "## Step 2A: Run Baseline (Original MKA - No Alpha Training)\n",
    "\n",
    "**Note:** You mentioned you already have baseline results for num_layer=13, so you can skip this step.\n",
    "\n",
    "If you need to run it again:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33ae34a4",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "This notebook compared:\n",
    "1. **Baseline MKA** (similarity-based heuristic S_lm)\n",
    "2. **Learned Scalar α** (trained via gradient descent)\n",
    "\n",
    "**Key Findings:**\n",
    "- Learned α values may differ from similarity scores\n",
    "- Check correlation between learned α and S_lm\n",
    "- Compare MMLU accuracy to see if learning improves performance\n",
    "\n",
    "**Next Steps:**\n",
    "- Try MLP-based dynamic merging (see `train_mlp_alpha.ipynb`)\n",
    "- Run full comparison with `evaluate_methods.py`"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
