{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "530afdd1",
   "metadata": {},
   "source": [
    "# Learnable Scalar Alpha Training for MKA\n",
    "\n",
    "Train and evaluate learnable scalar Œ± for layer merging."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "175aab97",
   "metadata": {},
   "source": [
    "## 1. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3edd7d45",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Logged in to HuggingFace\n",
      "============================================================\n",
      "SCALAR ALPHA EXPERIMENT - CONFIGURATION\n",
      "============================================================\n",
      "  Model: meta-llama/Meta-Llama-3-8B\n",
      "  Layers to merge: 13\n",
      "  Training steps: 500\n",
      "  Learning rate: 0.0001\n",
      "============================================================\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import subprocess\n",
    "from huggingface_hub import login\n",
    "\n",
    "# HuggingFace Authentication\n",
    "HF_TOKEN = \"\"\n",
    "\n",
    "if HF_TOKEN:\n",
    "    login(token=HF_TOKEN)\n",
    "    print(\"‚úì Logged in to HuggingFace\")\n",
    "\n",
    "# Configuration\n",
    "MODEL_PATH = \"meta-llama/Meta-Llama-3-8B\"\n",
    "DATA_DIR = \"./data\"\n",
    "NUM_LAYERS = 13  # Must match your baseline evaluation\n",
    "OUTPUT_DIR_BASELINE = \"./output_baseline\"\n",
    "OUTPUT_DIR_LEARNED = \"./output_learned\"\n",
    "\n",
    "# Training hyperparameters\n",
    "ALPHA_TRAINING_STEPS = 500\n",
    "ALPHA_LEARNING_RATE = 1e-4\n",
    "CALIBRATION_BATCH_SIZE = 4\n",
    "CALIBRATION_SAMPLES = 100\n",
    "\n",
    "print(\"=\" * 60)\n",
    "print(\"SCALAR ALPHA EXPERIMENT - CONFIGURATION\")\n",
    "print(\"=\" * 60)\n",
    "print(f\"  Model: {MODEL_PATH}\")\n",
    "print(f\"  Layers to merge: {NUM_LAYERS}\")\n",
    "print(f\"  Training steps: {ALPHA_TRAINING_STEPS}\")\n",
    "print(f\"  Learning rate: {ALPHA_LEARNING_RATE}\")\n",
    "print(\"=\" * 60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9c10f4c",
   "metadata": {},
   "source": [
    "## 2. Download MMLU Dataset (if needed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7cd054b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Data directory already exists\n"
     ]
    }
   ],
   "source": [
    "# Download MMLU dataset (only need to run once)\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "if not os.path.exists(\"./data\"):\n",
    "    print(\"üì• Downloading MMLU dataset...\")\n",
    "    # Clone the official MMLU repository\n",
    "    !git clone https://github.com/hendrycks/test.git mmlu_download\n",
    "    \n",
    "    # Move the data folder\n",
    "    !mv mmlu_download/data ./data\n",
    "    \n",
    "    # Clean up\n",
    "    !rm -rf mmlu_download\n",
    "    \n",
    "    # Verify structure\n",
    "    if os.path.exists(\"./data/dev\") and os.path.exists(\"./data/test\"):\n",
    "        print(\"‚úÖ MMLU dataset downloaded successfully!\")\n",
    "        dev_count = len([f for f in os.listdir(\"./data/dev\") if f.endswith(\"_dev.csv\")])\n",
    "        test_count = len([f for f in os.listdir(\"./data/test\") if f.endswith(\"_test.csv\")])\n",
    "        print(f\"   Dev files: {dev_count}, Test files: {test_count}\")\n",
    "    else:\n",
    "        print(\"‚ö†Ô∏è Download completed but structure looks wrong\")\n",
    "else:\n",
    "    print(\"‚úÖ Data directory already exists\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03311b2b",
   "metadata": {},
   "source": [
    "## 3. Verify Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "909933fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì Data directory exists: 57 dev files, 57 test files\n"
     ]
    }
   ],
   "source": [
    "# Check data directory\n",
    "if os.path.exists(DATA_DIR):\n",
    "    dev_files = os.listdir(os.path.join(DATA_DIR, \"dev\")) if os.path.exists(os.path.join(DATA_DIR, \"dev\")) else []\n",
    "    test_files = os.listdir(os.path.join(DATA_DIR, \"test\")) if os.path.exists(os.path.join(DATA_DIR, \"test\")) else []\n",
    "    print(f\"‚úì Data directory exists: {len(dev_files)} dev files, {len(test_files)} test files\")\n",
    "else:\n",
    "    print(f\"‚úó Data directory not found: {DATA_DIR}\")\n",
    "    print(\"  Make sure MMLU data is in ./data/dev/ and ./data/test/\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8ae111d5",
   "metadata": {},
   "source": [
    "## 4. Train Learnable Alpha (saves model, ~30 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c0f2e989",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "^C\n"
     ]
    }
   ],
   "source": [
    "# Train learnable alpha and evaluate on MMLU\n",
    "!python pipeline.py --model_path \"meta-llama/Meta-Llama-3-8B\" --num_layer 13 --data_dir \"./data\" --use_learnable_alpha --alpha_training_steps 500 --alpha_learning_rate 1e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94d45109",
   "metadata": {},
   "source": [
    "## 5. Load Fused Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7672518a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the fused model with learned alphas\n",
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "import os\n",
    "\n",
    "# Path to the saved fused model\n",
    "MODEL_DIR = f\"./output/Meta-Llama-3-8B/fused_{NUM_LAYERS}_layers/iteration/merged_weights\"\n",
    "\n",
    "print(\"Loading fused model with learned alphas...\")\n",
    "print(f\"Model directory: {MODEL_DIR}\")\n",
    "\n",
    "# Check if model exists\n",
    "if not os.path.exists(MODEL_DIR):\n",
    "    print(f\"‚ùå Model directory not found: {MODEL_DIR}\")\n",
    "    print(\"   Make sure training has completed successfully.\")\n",
    "else:\n",
    "    # Load tokenizer\n",
    "    tokenizer = AutoTokenizer.from_pretrained(\n",
    "        MODEL_PATH,\n",
    "        use_fast=True,\n",
    "        trust_remote_code=True,\n",
    "        padding_side=\"left\"\n",
    "    )\n",
    "    \n",
    "    if tokenizer.pad_token is None:\n",
    "        tokenizer.pad_token = tokenizer.eos_token\n",
    "        tokenizer.pad_token_id = tokenizer.eos_token_id\n",
    "    \n",
    "    # Load the fused model\n",
    "    fused_model = AutoModelForCausalLM.from_pretrained(\n",
    "        MODEL_DIR,\n",
    "        trust_remote_code=True,\n",
    "        device_map=\"auto\",\n",
    "        torch_dtype=torch.bfloat16 if torch.cuda.is_bf16_supported() else torch.float32,\n",
    "    )\n",
    "    \n",
    "    # Disable caching\n",
    "    fused_model.config.use_cache = False\n",
    "    \n",
    "    print(f\"‚úÖ Model loaded successfully!\")\n",
    "    print(f\"   Number of layers: {fused_model.config.num_hidden_layers}\")\n",
    "    print(f\"   Model dtype: {fused_model.dtype}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2240ce27",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate on MMLU\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import json\n",
    "\n",
    "# MMLU evaluation function\n",
    "choices = [\"A\", \"B\", \"C\", \"D\"]\n",
    "\n",
    "def format_subject(subject):\n",
    "    l = subject.split(\"_\")\n",
    "    s = \"\"\n",
    "    for entry in l:\n",
    "        s += \" \" + entry\n",
    "    return s\n",
    "\n",
    "def format_example(df, idx, include_answer=True):\n",
    "    prompt = df.iloc[idx, 0]\n",
    "    k = df.shape[1] - 2\n",
    "    for j in range(k):\n",
    "        prompt += \"\\n{}. {}\".format(choices[j], df.iloc[idx, j + 1])\n",
    "    prompt += \"\\nAnswer:\"\n",
    "    if include_answer:\n",
    "        prompt += \" {}\\n\\n\".format(df.iloc[idx, k + 1])\n",
    "    return prompt\n",
    "\n",
    "def gen_prompt(train_df, subject, k=-1):\n",
    "    prompt = \"The following are multiple choice questions (with answers) about {}.\\n\\n\".format(\n",
    "        format_subject(subject)\n",
    "    )\n",
    "    if k == -1:\n",
    "        k = train_df.shape[0]\n",
    "    for i in range(k):\n",
    "        prompt += format_example(train_df, i)\n",
    "    return prompt\n",
    "\n",
    "@torch.no_grad()\n",
    "def eval_subject(subject, model, tokenizer, dev_df, test_df, ntrain=5):\n",
    "    cors = []\n",
    "    total_loss = 0\n",
    "    \n",
    "    for i in tqdm(range(test_df.shape[0]), desc=f\"Evaluating {subject}\"):\n",
    "        prompt_end = format_example(test_df, i, include_answer=False)\n",
    "        train_prompt = gen_prompt(dev_df, subject, ntrain)\n",
    "        prompt = train_prompt + prompt_end\n",
    "        \n",
    "        input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.cuda()\n",
    "        labels = input_ids.clone()\n",
    "        labels[:, :-len(tokenizer(prompt_end).input_ids)] = -100\n",
    "        \n",
    "        outputs = model(input_ids=input_ids, labels=labels, use_cache=False)\n",
    "        logits = outputs.logits[:, -1, :]\n",
    "        loss = outputs.loss\n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        probs = torch.nn.functional.softmax(logits, dim=-1).detach().float().cpu().numpy()\n",
    "        pred = choices[np.argmax(probs[:, [tokenizer(c).input_ids[-1] for c in choices]])]\n",
    "        label = test_df.iloc[i, test_df.shape[1] - 1]\n",
    "        \n",
    "        cor = pred == label\n",
    "        cors.append(cor)\n",
    "    \n",
    "    acc = np.mean(cors)\n",
    "    avg_loss = total_loss / len(test_df)\n",
    "    ppl = np.exp(avg_loss)\n",
    "    \n",
    "    return acc, ppl\n",
    "\n",
    "# Run evaluation on all subjects\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"EVALUATING FUSED MODEL ON MMLU\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "fused_model.eval()\n",
    "\n",
    "subjects = sorted([\n",
    "    f.split(\"_test.csv\")[0]\n",
    "    for f in os.listdir(os.path.join(DATA_DIR, \"test\"))\n",
    "    if \"_test.csv\" in f\n",
    "])\n",
    "\n",
    "all_accs = {}\n",
    "all_ppls = {}\n",
    "\n",
    "for subject in subjects:\n",
    "    dev_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"dev\", subject + \"_dev.csv\"), header=None\n",
    "    )[:5]  # Use 5 examples\n",
    "    test_df = pd.read_csv(\n",
    "        os.path.join(DATA_DIR, \"test\", subject + \"_test.csv\"), header=None\n",
    "    )\n",
    "    \n",
    "    acc, ppl = eval_subject(subject, fused_model, tokenizer, dev_df, test_df, ntrain=5)\n",
    "    \n",
    "    all_accs[subject] = acc\n",
    "    all_ppls[subject] = ppl\n",
    "    \n",
    "    print(f\"Average accuracy {acc:.3f} - {subject}\")\n",
    "    print(f\"Perplexity {ppl:.3f} - {subject}\")\n",
    "\n",
    "avg_acc = np.mean(list(all_accs.values()))\n",
    "avg_ppl = np.mean(list(all_ppls.values()))\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\"MMLU EVALUATION RESULTS (LEARNED ALPHA)\")\n",
    "print(\"=\"*60)\n",
    "print(f\"Average Accuracy:   {avg_acc:.4f}\")\n",
    "print(f\"Average Perplexity: {avg_ppl:.4f}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Save results\n",
    "results = {\n",
    "    \"average_accuracy\": float(avg_acc),\n",
    "    \"average_perplexity\": float(avg_ppl),\n",
    "    \"per_subject_accuracy\": {k: float(v) for k, v in all_accs.items()},\n",
    "    \"per_subject_perplexity\": {k: float(v) for k, v in all_ppls.items()},\n",
    "}\n",
    "\n",
    "results_path = f\"./output/Meta-Llama-3-8B/fused_{NUM_LAYERS}_layers/iteration/fusion_info/mmlu_results.json\"\n",
    "os.makedirs(os.path.dirname(results_path), exist_ok=True)\n",
    "with open(results_path, \"w\") as f:\n",
    "    json.dump(results, f, indent=2)\n",
    "\n",
    "print(f\"\\n‚úÖ Results saved to: {results_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e70cd306",
   "metadata": {},
   "source": [
    "## 6. Evaluate on MMLU (~40-90 min)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21570d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize top and bottom performing subjects\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Sort subjects by accuracy\n",
    "sorted_subjects = sorted(all_accs.items(), key=lambda x: x[1], reverse=True)\n",
    "\n",
    "# Get top 10 and bottom 10\n",
    "top_10 = sorted_subjects[:10]\n",
    "bottom_10 = sorted_subjects[-10:]\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# Top 10 subjects\n",
    "subjects_top = [s[0] for s in top_10]\n",
    "accs_top = [s[1] for s in top_10]\n",
    "\n",
    "ax1.barh(range(len(subjects_top)), accs_top, color='green', alpha=0.7)\n",
    "ax1.set_yticks(range(len(subjects_top)))\n",
    "ax1.set_yticklabels(subjects_top, fontsize=10)\n",
    "ax1.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax1.set_title('Top 10 Subjects by Accuracy', fontsize=14, fontweight='bold')\n",
    "ax1.invert_yaxis()\n",
    "ax1.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, acc in enumerate(accs_top):\n",
    "    ax1.text(acc + 0.01, i, f'{acc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "# Bottom 10 subjects\n",
    "subjects_bottom = [s[0] for s in bottom_10]\n",
    "accs_bottom = [s[1] for s in bottom_10]\n",
    "\n",
    "ax2.barh(range(len(subjects_bottom)), accs_bottom, color='red', alpha=0.7)\n",
    "ax2.set_yticks(range(len(subjects_bottom)))\n",
    "ax2.set_yticklabels(subjects_bottom, fontsize=10)\n",
    "ax2.set_xlabel('Accuracy', fontsize=12, fontweight='bold')\n",
    "ax2.set_title('Bottom 10 Subjects by Accuracy', fontsize=14, fontweight='bold')\n",
    "ax2.invert_yaxis()\n",
    "ax2.grid(axis='x', alpha=0.3)\n",
    "\n",
    "# Add accuracy values on bars\n",
    "for i, acc in enumerate(accs_bottom):\n",
    "    ax2.text(acc + 0.01, i, f'{acc:.3f}', va='center', fontsize=10)\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(f\"\\nüìä Overall Average Accuracy: {avg_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb2103a",
   "metadata": {},
   "source": [
    "## 7. Visualize Top/Bottom Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "312e36f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Load learned alphas\n",
    "learned_alphas_path = \"./output/Meta-Llama-3-8B/fused_13_layers/iteration/merged_weights/learned_alphas.json\"\n",
    "\n",
    "if os.path.exists(learned_alphas_path):\n",
    "    with open(learned_alphas_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    learned_alphas = data.get('learned_alphas', [])\n",
    "    similarity_scores = data.get('similarity_scores', [])\n",
    "    \n",
    "    print(\"=\" * 60)\n",
    "    print(\"LEARNED ALPHA STATISTICS\")\n",
    "    print(\"=\" * 60)\n",
    "    print(f\"  Number of layers: {len(learned_alphas)}\")\n",
    "    print(f\"  Mean Œ±: {np.mean(learned_alphas):.4f}\")\n",
    "    print(f\"  Std Œ±:  {np.std(learned_alphas):.4f}\")\n",
    "    print(f\"  Min Œ±:  {np.min(learned_alphas):.4f}\")\n",
    "    print(f\"  Max Œ±:  {np.max(learned_alphas):.4f}\")\n",
    "    print(\"=\" * 60)\n",
    "    \n",
    "    # Visualize alpha distribution\n",
    "    plt.figure(figsize=(12, 4))\n",
    "    \n",
    "    plt.subplot(1, 3, 1)\n",
    "    plt.hist(learned_alphas, bins=15, edgecolor='black', alpha=0.7, color='steelblue')\n",
    "    plt.axvline(np.mean(learned_alphas), color='r', linestyle='--', label=f'Mean: {np.mean(learned_alphas):.3f}')\n",
    "    plt.xlabel('Alpha Value')\n",
    "    plt.ylabel('Frequency')\n",
    "    plt.title('Distribution of Learned Œ±')\n",
    "    plt.legend()\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs layer index\n",
    "    plt.subplot(1, 3, 2)\n",
    "    plt.plot(range(len(learned_alphas)), learned_alphas, marker='o', linestyle='-', color='darkgreen')\n",
    "    plt.xlabel('Layer Index')\n",
    "    plt.ylabel('Learned Œ±')\n",
    "    plt.title('Learned Œ± Across Layers')\n",
    "    plt.grid(alpha=0.3)\n",
    "    \n",
    "    # Alpha vs Similarity\n",
    "    if similarity_scores and len(similarity_scores) == len(learned_alphas):\n",
    "        plt.subplot(1, 3, 3)\n",
    "        plt.scatter(similarity_scores, learned_alphas, alpha=0.6, s=100, color='coral')\n",
    "        plt.xlabel('Similarity Score (S_lm)')\n",
    "        plt.ylabel('Learned Œ±')\n",
    "        plt.title('Learned Œ± vs Similarity')\n",
    "        corr = np.corrcoef(similarity_scores, learned_alphas)[0, 1]\n",
    "        plt.text(0.05, 0.95, f'Correlation: {corr:.3f}', transform=plt.gca().transAxes, \n",
    "                bbox=dict(boxstyle='round', facecolor='wheat', alpha=0.5))\n",
    "        plt.grid(alpha=0.3)\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n‚úì Analysis complete! Check the plots above.\")\n",
    "else:\n",
    "    print(f\"‚úó Learned alphas not found: {learned_alphas_path}\")\n",
    "    print(\"  Training may still be running.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eac19a5",
   "metadata": {},
   "source": [
    "## 8. Analyze Learned Alpha Values"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
